{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "accum_optimizer_for_keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRdAGE-TFIIn",
        "colab_type": "text"
      },
      "source": [
        "Keras梯度累积优化器：用时间换取效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h1P93SMIDwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4169edb0-0d90-40cc-eb20-0404d3fe58c3"
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "\n",
        "from keras.optimizers import Optimizer\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "class AccumOptimizer(Optimizer):\n",
        "    \"\"\"继承Optimizer类，包装原有优化器，实现梯度累积。\n",
        "    # 参数\n",
        "        optimizer：优化器实例，支持目前所有的keras优化器；\n",
        "        steps_per_update：累积的步数。\n",
        "    # 返回\n",
        "        一个新的keras优化器\n",
        "    Inheriting Optimizer class, wrapping the original optimizer\n",
        "    to achieve a new corresponding optimizer of gradient accumulation.\n",
        "    # Arguments\n",
        "        optimizer: an instance of keras optimizer (supporting\n",
        "                    all keras optimizers currently available);\n",
        "        steps_per_update: the steps of gradient accumulation\n",
        "    # Returns\n",
        "        a new keras optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, steps_per_update=1, **kwargs):\n",
        "        super(AccumOptimizer, self).__init__(**kwargs)\n",
        "        self.optimizer = optimizer\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.steps_per_update = steps_per_update\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.cond = K.equal(self.iterations % self.steps_per_update, 0)\n",
        "            self.lr = self.optimizer.lr\n",
        "            self.optimizer.lr = K.switch(self.cond, self.optimizer.lr, 0.)\n",
        "            for attr in ['momentum', 'rho', 'beta_1', 'beta_2']:\n",
        "                if hasattr(self.optimizer, attr):\n",
        "                    value = getattr(self.optimizer, attr)\n",
        "                    setattr(self, attr, value)\n",
        "                    setattr(self.optimizer, attr, K.switch(self.cond, value, 1 - 1e-7))\n",
        "            for attr in self.optimizer.get_config():\n",
        "                if not hasattr(self, attr):\n",
        "                    value = getattr(self.optimizer, attr)\n",
        "                    setattr(self, attr, value)\n",
        "            # 覆盖原有的获取梯度方法，指向累积梯度\n",
        "            # Cover the original get_gradients method with accumulative gradients.\n",
        "            def get_gradients(loss, params):\n",
        "                return [ag / self.steps_per_update for ag in self.accum_grads]\n",
        "            self.optimizer.get_gradients = get_gradients\n",
        "    def get_updates(self, loss, params):\n",
        "        self.updates = [\n",
        "            K.update_add(self.iterations, 1),\n",
        "            K.update_add(self.optimizer.iterations, K.cast(self.cond, 'int64')),\n",
        "        ]\n",
        "        # 累积梯度 (gradient accumulation)\n",
        "        self.accum_grads = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        for g, ag in zip(grads, self.accum_grads):\n",
        "            self.updates.append(K.update(ag, K.switch(self.cond, ag * 0, ag + g)))\n",
        "        # 继承optimizer的更新 (inheriting updates of original optimizer)\n",
        "        self.updates.extend(self.optimizer.get_updates(loss, params)[1:])\n",
        "        self.weights.extend(self.optimizer.weights)\n",
        "        return self.updates\n",
        "    def get_config(self):\n",
        "        iterations = K.eval(self.iterations)\n",
        "        K.set_value(self.iterations, 0)\n",
        "        config = self.optimizer.get_config()\n",
        "        K.set_value(self.iterations, iterations)\n",
        "        return config"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Bgjx-zFAUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ef2c838-eaa7-4431-ca64-bacba577d643"
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "#from accum_optimizer import AccumOptimizer\n",
        "\n",
        "\n",
        "batch_size = 12800\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=AccumOptimizer(Adam(), 100), # equals batch_size=100\n",
        "              #optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=100,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 1.6315 - acc: 0.5491 - val_loss: 0.7542 - val_acc: 0.8294\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.6277 - acc: 0.8205 - val_loss: 0.3948 - val_acc: 0.8842\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.4316 - acc: 0.8696 - val_loss: 0.3256 - val_acc: 0.9053\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3596 - acc: 0.8956 - val_loss: 0.2723 - val_acc: 0.9224\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3016 - acc: 0.9118 - val_loss: 0.2338 - val_acc: 0.9318\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2551 - acc: 0.9250 - val_loss: 0.2045 - val_acc: 0.9396\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2267 - acc: 0.9340 - val_loss: 0.1863 - val_acc: 0.9453\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2042 - acc: 0.9409 - val_loss: 0.1684 - val_acc: 0.9495\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1833 - acc: 0.9460 - val_loss: 0.1547 - val_acc: 0.9534\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1650 - acc: 0.9520 - val_loss: 0.1423 - val_acc: 0.9571\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.1533 - acc: 0.9549 - val_loss: 0.1317 - val_acc: 0.9597\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1423 - acc: 0.9577 - val_loss: 0.1227 - val_acc: 0.9626\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1303 - acc: 0.9614 - val_loss: 0.1155 - val_acc: 0.9645\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.1207 - acc: 0.9648 - val_loss: 0.1084 - val_acc: 0.9667\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.1136 - acc: 0.9663 - val_loss: 0.1033 - val_acc: 0.9683\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1063 - acc: 0.9686 - val_loss: 0.0980 - val_acc: 0.9699\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0990 - acc: 0.9709 - val_loss: 0.0945 - val_acc: 0.9711\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0929 - acc: 0.9721 - val_loss: 0.0892 - val_acc: 0.9717\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0877 - acc: 0.9736 - val_loss: 0.0864 - val_acc: 0.9733\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0827 - acc: 0.9756 - val_loss: 0.0832 - val_acc: 0.9742\n",
            "Test loss: 0.08319587814202532\n",
            "Test accuracy: 0.9742\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}